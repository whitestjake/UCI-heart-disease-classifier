{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category features\n",
    "cont_cols  = ['age', 'chol', 'oldpeak', 'thalch', 'trestbps']\n",
    "cat_cols = ['ca', 'cp', 'restecg', 'slope', 'thal', 'sex', 'fbs', 'exang']\n",
    "\n",
    "#read training data, normalize continous features using (X - mean) / std \n",
    "df = pd.read_csv('heart_disease_uci.csv')\n",
    "\n",
    "# Create a new dataframe from original and change all target variables greater than 0 to 1, instead of 1,2,3,4,5\n",
    "df_fix = df\n",
    "df_fix['num'] = np.where(df_fix['num'] > 0, 1, 0)\n",
    "\n",
    "# Seperate features (X) and target (y), removing id, num and dataset from X\n",
    "X = df_fix.drop(columns=['id', 'num', 'dataset'])\n",
    "y = df_fix['num']\n",
    "\n",
    "# Replace missing data in categorical columns with unknown\n",
    "for cols in cat_cols:\n",
    "    X[cols] = X[cols].fillna('Unknown')\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "# Convert continuous column values to float\n",
    "X[cont_cols] = X.loc[:, cont_cols].astype('float64')\n",
    "\n",
    "# find the mean and standard deviation of each continuous column\n",
    "train_means = X[cont_cols].mean()\n",
    "train_std = X[cont_cols].std()\n",
    "\n",
    "# replace any missing value in a continuous column with the mean of that column\n",
    "X.loc[:, cont_cols] = X[cont_cols].fillna(train_means)\n",
    "\n",
    "# Normalize data\n",
    "X.loc[:, cont_cols] = (X.loc[:, cont_cols] - train_means) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## KNN CLASSIFIER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling data before splitting for randomized train, valid, test sets\n",
    "shuffled = df_fix.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# separate data into training, validation, and testing sets\n",
    "train_split = int(0.6 * len(X))\n",
    "test_split = int(train_split + (0.2 * len(X)))\n",
    "X_train = X[:train_split]\n",
    "X_valid = X[train_split:test_split]\n",
    "X_test = X[test_split:]\n",
    "\n",
    "y_train = y[:train_split]\n",
    "y_valid = y[train_split:test_split]\n",
    "y_test = y[test_split:]\n",
    "\n",
    "# convert to numpy arrays to more easily apply k-NN formulas\n",
    "X_train = X_train.values\n",
    "X_valid = X_valid.values\n",
    "X_test = X_test.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_valid = y_valid.values\n",
    "y_test = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the L2 distance between the current row and a neighbor\n",
    "def l2_distance(x, X_train):\n",
    "    return np.sum((X_train - x)**2, axis=1)\n",
    "    \n",
    "# get the K closest neighbors\n",
    "def knn_predict(x, X_train, y_train, k):\n",
    "    distances = l2_distance(x, X_train)\n",
    "    idx = np.argpartition(distances, k)[:k]  # indices of k nearest\n",
    "    neighbor_labels = y_train[idx]\n",
    "    counts = Counter(neighbor_labels.tolist()).most_common(1)  \n",
    "    return counts[0][0]  # majority vote\n",
    "\n",
    "# loop over all rows\n",
    "def knn_predict_batch(X_test, X_train, y_train, k):\n",
    "    preds = [knn_predict(x, X_train, y_train, k) for x in X_test]\n",
    "    return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(tp, fp, tn, fn):\n",
    "    print(\"\\n%15sActual\" % \"\")\n",
    "    print(\"%6s %7s %7s\" % (\"\", \"1\", \"0\"))\n",
    "    print(\"P%6s +--------+--------+\" % \"\")\n",
    "    print(\"r%6s | %-6s | %-6s |\" % (\"1\", 'TP='+str(tp), 'FP='+str(fp)))\n",
    "    print(\"e%6s +--------+--------+\" % \"\")\n",
    "    print(\"d%6s | %-6s | %-6s |\" % (\"0\", 'FN='+str(fn), 'TN='+str(tn)))\n",
    "    print(\".%6s +--------+--------+\\n\" % \"\")\n",
    "\n",
    "def getConfusionMatrix(y_true, y_pred):\n",
    "\n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    \n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def getAccuracy(tp, fp, tn, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def getPrecision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def getRecall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def getFScore(tp, fp, tn, fn, beta=1):\n",
    "    precision = getPrecision(tp, fp, tn, fn)\n",
    "    recall = getRecall(tp, fp, tn, fn)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 13\n",
      "Validation Accuracy: 0.5706521739130435\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning k value on validation set\n",
    "best_k, best_score = None, -1\n",
    "\n",
    "for k in range(1, 25, 2):\n",
    "    preds = knn_predict_batch(X_valid, X_train, y_train, k)\n",
    "    acc = (preds == y_valid).mean()\n",
    "    \n",
    "    if acc > best_score:\n",
    "        best_score, best_k = acc, k\n",
    "\n",
    "print('Best k:', best_k)\n",
    "print('Validation Accuracy:', best_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               Actual\n",
      "             1       0\n",
      "P       +--------+--------+\n",
      "r     1 | TP=83  | FP=9   |\n",
      "e       +--------+--------+\n",
      "d     0 | FN=56  | TN=36  |\n",
      ".       +--------+--------+\n",
      "\n",
      "Accuracy:   0.64674\n",
      "Precison:   0.90217\n",
      "Recall:     0.59712\n",
      "F1 Score:   0.71861\n"
     ]
    }
   ],
   "source": [
    "# report final performance\n",
    "\n",
    "tp, fp, tn, fn = getConfusionMatrix(y_test, knn_predict_batch(X_test, X_train, y_train, best_k))\n",
    "printConfusionMatrix(tp, fp, tn, fn)\n",
    "                \n",
    "print('Accuracy:  %8.5f' % getAccuracy(tp, fp, tn, fn))\n",
    "print('Precison:  %8.5f' % getPrecision(tp, fp, tn, fn))\n",
    "print('Recall:    %8.5f' % getRecall(tp, fp, tn, fn))\n",
    "print('F1 Score:  %8.5f' % getFScore(tp, fp, tn, fn))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
